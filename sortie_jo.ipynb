{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokeniseur très simple\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "# Construction vocabulaire + encodage\n",
    "def build_vocab(reviews, vocab_size=5000, min_freq=5):\n",
    "    word_counts = {}\n",
    "    for text in reviews:\n",
    "        for word in tokenize(text):\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: -x[1])\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, count in sorted_words:\n",
    "        if count >= min_freq and len(vocab) < vocab_size:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(text, vocab, max_len=400):\n",
    "    tokens = tokenize(text)\n",
    "    indices = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens[:max_len]]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [vocab[\"<PAD>\"]] * (max_len - len(indices))\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = encode(self.texts[idx], self.vocab)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(text), torch.tensor(label, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentAwareEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.biases = nn.Parameter(torch.zeros(vocab_size))  # b_w dans le papier\n",
    "        self.sentiment_regressor = nn.Linear(embedding_dim, 1)  # ψ et b_c\n",
    "\n",
    "    def semantic_energy(self, word_vecs, theta):\n",
    "        return torch.matmul(word_vecs, theta.T)  # θᵗ · φ_w\n",
    "\n",
    "    def forward(self, docs, thetas):\n",
    "        # docs: [batch, max_len], thetas: [batch, embed_dim]\n",
    "        word_vecs = self.embeddings(docs)                      # [B, L, D]\n",
    "        avg_vec = word_vecs.mean(dim=1)                        # φ_doc ≈ moyenne(φ_w)\n",
    "        sentiment_logits = self.sentiment_regressor(avg_vec).squeeze(1)\n",
    "        sentiment_probs = torch.sigmoid(sentiment_logits)\n",
    "        return sentiment_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for docs, labels in train_loader:\n",
    "            batch_size = docs.shape[0]\n",
    "            theta = torch.randn(batch_size, model.embeddings.embedding_dim, requires_grad=True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(docs, theta)\n",
    "            loss = F.binary_cross_entropy(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            acc = ((preds > 0.5) == labels).float().mean()\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f} - Acc: {total_acc / len(train_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
