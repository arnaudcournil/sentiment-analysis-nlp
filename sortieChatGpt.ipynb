{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d3227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 800, 200)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 500\n",
    "embedding_dim = 50\n",
    "n_docs = 1000\n",
    "doc_length = 40\n",
    "\n",
    "# Simulate a vocabulary: 0 to 499\n",
    "# Create some words associated with sentiment\n",
    "positive_words = set(random.sample(range(vocab_size), 50))  # 10% of vocab\n",
    "negative_words = set(random.sample([w for w in range(vocab_size) if w not in positive_words], 50))\n",
    "\n",
    "# Function to generate a document\n",
    "def generate_doc(label):\n",
    "    doc = []\n",
    "    for _ in range(doc_length):\n",
    "        if label == 1 and random.random() < 0.3:\n",
    "            doc.append(random.choice(list(positive_words)))\n",
    "        elif label == 0 and random.random() < 0.3:\n",
    "            doc.append(random.choice(list(negative_words)))\n",
    "        else:\n",
    "            doc.append(random.randint(0, vocab_size - 1))\n",
    "    return doc\n",
    "\n",
    "# Generate corpus and labels\n",
    "corpus = []\n",
    "labels = []\n",
    "for _ in range(n_docs):\n",
    "    sentiment = random.randint(0, 1)\n",
    "    doc = generate_doc(sentiment)\n",
    "    corpus.append(doc)\n",
    "    labels.append(sentiment)\n",
    "\n",
    "# Split into train and test\n",
    "corpus_train, corpus_test, labels_train, labels_test = train_test_split(corpus, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "len(corpus_train), len(corpus_test), len(labels_train), len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c28b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 218659.3347\n",
      "Epoch 2, Loss: 492718.7797\n",
      "Epoch 3, Loss: 499190.3542\n",
      "Epoch 4, Loss: 564576.5023\n",
      "Epoch 5, Loss: 2725768.1949\n",
      "Test Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Sentiment-Aware Word Vector Learning\n",
    "# Based on \"Learning Word Vectors for Sentiment Analysis\" (Maas et al., ACL 2011)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax, expit as sigmoid\n",
    "\n",
    "class SentimentWordVectorModel:\n",
    "    def __init__(self, vocab_size, embedding_dim, alpha=1e-4):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.phi = np.random.randn(vocab_size, embedding_dim) * 0.01  # word vectors\n",
    "        self.b_w = np.zeros(vocab_size)  # word biases\n",
    "        self.psi = np.random.randn(embedding_dim) * 0.01  # sentiment classifier weights\n",
    "        self.b_c = 0.0  # sentiment classifier bias\n",
    "\n",
    "    def document_log_likelihood(self, doc_word_indices, theta):\n",
    "        logits = theta @ self.phi.T + self.b_w\n",
    "        log_probs = logits - np.log(np.sum(np.exp(logits)))\n",
    "        return np.sum(log_probs[doc_word_indices])\n",
    "\n",
    "    def sentiment_log_likelihood(self, doc_word_indices, sentiment_label):\n",
    "        word_vecs = self.phi[doc_word_indices]\n",
    "        logits = word_vecs @ self.psi + self.b_c\n",
    "        probs = sigmoid(logits)\n",
    "        if sentiment_label == 1:\n",
    "            return np.sum(np.log(probs + 1e-8))\n",
    "        else:\n",
    "            return np.sum(np.log(1 - probs + 1e-8))\n",
    "\n",
    "    def total_loss(self, corpus, sentiment_labels, thetas):\n",
    "        total = 0\n",
    "        for i, doc in enumerate(corpus):\n",
    "            total += self.document_log_likelihood(doc, thetas[i])\n",
    "            total += self.sentiment_log_likelihood(doc, sentiment_labels[i])\n",
    "        # Add L2 regularization\n",
    "        total -= self.alpha * (np.sum(self.phi**2) + np.sum(self.psi**2))\n",
    "        return -total  # negative log-likelihood\n",
    "\n",
    "    def fit(self, corpus, sentiment_labels, n_iters=10):\n",
    "        N = len(corpus)\n",
    "        thetas = np.random.randn(N, self.embedding_dim) * 0.01\n",
    "\n",
    "        for epoch in range(n_iters):\n",
    "            for i, doc in enumerate(corpus):\n",
    "                theta = thetas[i]\n",
    "                grad_theta = self._grad_theta(doc, theta)\n",
    "                thetas[i] += 0.01 * grad_theta\n",
    "\n",
    "            grad_phi, grad_b_w, grad_psi, grad_b_c = self._grad_global(corpus, sentiment_labels, thetas)\n",
    "            self.phi += 0.01 * grad_phi\n",
    "            self.b_w += 0.01 * grad_b_w\n",
    "            self.psi += 0.01 * grad_psi\n",
    "            self.b_c += 0.01 * grad_b_c\n",
    "\n",
    "            loss = self.total_loss(corpus, sentiment_labels, thetas)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "        self.thetas = thetas  # Save document representations\n",
    "        return self\n",
    "\n",
    "    def _grad_theta(self, doc_indices, theta):\n",
    "        logits = theta @ self.phi.T + self.b_w\n",
    "        probs = softmax(logits)\n",
    "        grad = np.zeros_like(theta)\n",
    "        for i in doc_indices:\n",
    "            grad += self.phi[i] - (probs @ self.phi)\n",
    "        return grad - self.alpha * theta\n",
    "\n",
    "    def _grad_global(self, corpus, sentiment_labels, thetas):\n",
    "        grad_phi = np.zeros_like(self.phi)\n",
    "        grad_b_w = np.zeros_like(self.b_w)\n",
    "        grad_psi = np.zeros_like(self.psi)\n",
    "        grad_b_c = 0.0\n",
    "\n",
    "        for i, doc in enumerate(corpus):\n",
    "            theta = thetas[i]\n",
    "            logits = theta @ self.phi.T + self.b_w\n",
    "            probs = softmax(logits)\n",
    "\n",
    "            for w in range(self.vocab_size):\n",
    "                count = doc.count(w)\n",
    "                if count > 0:\n",
    "                    grad_phi[w] += count * (theta - (probs @ self.phi))\n",
    "                    grad_b_w[w] += count * (1 - probs[w])\n",
    "\n",
    "            word_vecs = self.phi[doc]\n",
    "            logits = word_vecs @ self.psi + self.b_c\n",
    "            probs = sigmoid(logits)\n",
    "            error = (sentiment_labels[i] - probs)\n",
    "            grad_phi_doc = error[:, None] * self.psi[None, :]\n",
    "            np.add.at(grad_phi, doc, grad_phi_doc)\n",
    "            grad_psi += np.sum(error[:, None] * word_vecs, axis=0)\n",
    "            grad_b_c += np.sum(error)\n",
    "\n",
    "        grad_phi -= self.alpha * self.phi\n",
    "        grad_psi -= self.alpha * self.psi\n",
    "        return grad_phi, grad_b_w, grad_psi, grad_b_c\n",
    "\n",
    "# === Entraînement ===\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    model = SentimentWordVectorModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "    model.fit(corpus_train, labels_train, n_iters=5)\n",
    "\n",
    "    # Évaluer le modèle avec une régression logistique sur les représentations de documents\n",
    "    X_train = model.thetas\n",
    "    y_train = np.array(labels_train)\n",
    "    X_test = np.array([np.mean(model.phi[doc], axis=0) for doc in corpus_test])\n",
    "    y_test = np.array(labels_test)\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
