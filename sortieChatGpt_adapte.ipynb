{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8680bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_imdb_data(imdb_path, split=\"train\", max_per_class=None):\n",
    "    \"\"\"\n",
    "    Charge les reviews et labels depuis aclImdb/train ou aclImdb/test.\n",
    "    Labels : 1 pour positif, 0 pour négatif.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in ['pos', 'neg']:\n",
    "        path = os.path.join(imdb_path, split, label)\n",
    "        files = os.listdir(path)\n",
    "        if max_per_class:\n",
    "            files = files[:max_per_class]\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, file), encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# Exemple d'utilisation\n",
    "imdb_path = \"aclImdb\"  # <- Remplace par le chemin réel\n",
    "train_texts, train_labels = load_imdb_data(imdb_path, split=\"train\", max_per_class=1000)\n",
    "test_texts, test_labels = load_imdb_data(imdb_path, split=\"test\", max_per_class=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cadf800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "def build_vocab(texts, max_vocab=5000, min_freq=5):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, freq in counter.most_common():\n",
    "        if freq >= min_freq and len(vocab) < max_vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(text, vocab, max_len=400):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "vocab = build_vocab(train_texts)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=400):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = encode(self.texts[idx], self.vocab, self.max_len)\n",
    "        label = float(self.labels[idx])\n",
    "        return torch.tensor(encoded), torch.tensor(label)\n",
    "\n",
    "train_dataset = IMDBDataset(train_texts, train_labels, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class MaasWordVectorModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "        self.sentiment_head = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x, theta):\n",
    "        emb = self.embedding(x)\n",
    "        avg_emb = emb.mean(dim=1)\n",
    "        sentiment_logits = self.sentiment_head(avg_emb).squeeze(1)\n",
    "        sentiment_probs = torch.sigmoid(sentiment_logits)\n",
    "\n",
    "        logits = torch.matmul(self.embedding.weight, theta.T).T + self.bias\n",
    "        return sentiment_probs, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99d65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, vocab_size, embedding_dim,\n",
    "          sentiment_weight=1.0, semantic_weight=1.0, \n",
    "          lambda_theta=1e-3, nu_R=1e-5, epochs=5, device=\"cpu\"):\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            theta = torch.randn(batch_size, embedding_dim, requires_grad=True, device=device)\n",
    "            theta_optimizer = torch.optim.SGD([theta], lr=0.1)\n",
    "\n",
    "            for _ in range(5):\n",
    "                _, logits = model(x, theta)\n",
    "                flat_x = x.view(-1)\n",
    "                flat_logits = logits.repeat_interleave(x.size(1), dim=0)\n",
    "                semantic_loss = F.cross_entropy(flat_logits, flat_x, reduction='mean')\n",
    "                reg_theta = lambda_theta * torch.norm(theta, p=2) ** 2\n",
    "                loss = semantic_loss + reg_theta\n",
    "                theta_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                theta_optimizer.step()\n",
    "\n",
    "            sentiment_probs, logits = model(x, theta.detach())\n",
    "            sentiment_loss = F.binary_cross_entropy(sentiment_probs, y)\n",
    "\n",
    "            flat_x = x.view(-1)\n",
    "            flat_logits = logits.repeat_interleave(x.size(1), dim=0)\n",
    "            semantic_loss = F.cross_entropy(flat_logits, flat_x, reduction='mean')\n",
    "\n",
    "            R = model.embedding.weight\n",
    "            reg_R = nu_R * torch.norm(R, p='fro') ** 2\n",
    "\n",
    "            total_batch_loss = (\n",
    "                sentiment_weight * sentiment_loss +\n",
    "                semantic_weight * semantic_loss +\n",
    "                reg_R\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = ((sentiment_probs > 0.5) == y).float().mean()\n",
    "            total_loss += total_batch_loss.item()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f} | Acc: {total_acc / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2dd407",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_acc\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(data_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation après entraînement :\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m evaluate(model, test_loader, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# === Évaluation sur données de test ===\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, vocab)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "def evaluate(model, data_loader, embedding_dim, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            theta = torch.randn(batch_size, embedding_dim).to(device)\n",
    "            sentiment_probs, _ = model(x, theta)\n",
    "            acc = ((sentiment_probs > 0.5) == y).float().mean()\n",
    "            total_acc += acc.item()\n",
    "    print(f\"Test Accuracy: {total_acc / len(data_loader):.4f}\")\n",
    "\n",
    "# Exemple d'utilisation après entraînement :\n",
    "evaluate(model, test_loader, embedding_dim=50, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab304d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
