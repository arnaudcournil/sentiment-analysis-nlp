{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09bc0586",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5548f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2a885",
   "metadata": {},
   "source": [
    "## Pré-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96ee479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"aclImdb\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "def load_reviews_from_dir(directory, label):\n",
    "    data = []\n",
    "    for fname in os.listdir(directory):\n",
    "        if fname.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, fname), encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                data.append((text, label))\n",
    "    return data\n",
    "\n",
    "def load_all_data():\n",
    "    train_pos = load_reviews_from_dir(os.path.join(train_dir, \"pos\"), 1)\n",
    "    train_neg = load_reviews_from_dir(os.path.join(train_dir, \"neg\"), 0)\n",
    "    test_pos  = load_reviews_from_dir(os.path.join(test_dir,  \"pos\"), 1)\n",
    "    test_neg  = load_reviews_from_dir(os.path.join(test_dir,  \"neg\"), 0)\n",
    "    return train_pos + train_neg + test_pos + test_neg, len(train_pos) + len(train_neg), len(test_pos) + len(test_neg)\n",
    "\n",
    "raw_data, len_train, len_test = load_all_data()\n",
    "df = pd.DataFrame(raw_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<br\\s*/?>\", \" \", text)  # remplace <br> et <br /> par un espace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # normalise les espaces\n",
    "    return text.strip()\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5050, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "vectorizer.fit(df[\"text\"])\n",
    "full_vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# --- Exclusion des 50 mots les plus fréquents comme dans l'article ---\n",
    "excluded_words = full_vocab[:50]\n",
    "vocab = full_vocab[50:]  # Top 5000 mots après exclusion des 50 premiers\n",
    "\n",
    "def filter_tokens(text, vocab_set):\n",
    "    tokens = text.split()\n",
    "    return \" \".join([tok for tok in tokens if tok in vocab_set])\n",
    "\n",
    "vocab_set = set(vocab)\n",
    "df[\"filtered_text\"] = df[\"text\"].apply(lambda x: filter_tokens(x, vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e99e88b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "      <td>is a cartoon ran at the same time as some othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "      <td>as has been an issue for years but never a pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "      <td>by dramatic lady have ever and love scenes in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "      <td>is easily the most underrated film the its doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>is not the typical was much less slapstick tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I occasionally let my kids watch this garbage ...</td>\n",
       "      <td>0</td>\n",
       "      <td>occasionally let my kids watch this garbage so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>When all we have anymore is pretty much realit...</td>\n",
       "      <td>0</td>\n",
       "      <td>all we have anymore is pretty much reality sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>The basic genre is a thriller intercut with an...</td>\n",
       "      <td>0</td>\n",
       "      <td>basic genre is a thriller with an uncomfortabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Four things intrigued me as to this film - fir...</td>\n",
       "      <td>0</td>\n",
       "      <td>things intrigued me as to this film it stars w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>David Bryce's comments nearby are exceptionall...</td>\n",
       "      <td>0</td>\n",
       "      <td>comments nearby are exceptionally well written...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      Bromwell High is a cartoon comedy. It ran at t...      1   \n",
       "1      Homelessness (or Houselessness as George Carli...      1   \n",
       "2      Brilliant over-acting by Lesley Ann Warren. Be...      1   \n",
       "3      This is easily the most underrated film inn th...      1   \n",
       "4      This is not the typical Mel Brooks film. It wa...      1   \n",
       "...                                                  ...    ...   \n",
       "49995  I occasionally let my kids watch this garbage ...      0   \n",
       "49996  When all we have anymore is pretty much realit...      0   \n",
       "49997  The basic genre is a thriller intercut with an...      0   \n",
       "49998  Four things intrigued me as to this film - fir...      0   \n",
       "49999  David Bryce's comments nearby are exceptionall...      0   \n",
       "\n",
       "                                           filtered_text  \n",
       "0      is a cartoon ran at the same time as some othe...  \n",
       "1      as has been an issue for years but never a pla...  \n",
       "2      by dramatic lady have ever and love scenes in ...  \n",
       "3      is easily the most underrated film the its doe...  \n",
       "4      is not the typical was much less slapstick tha...  \n",
       "...                                                  ...  \n",
       "49995  occasionally let my kids watch this garbage so...  \n",
       "49996  all we have anymore is pretty much reality sho...  \n",
       "49997  basic genre is a thriller with an uncomfortabl...  \n",
       "49998  things intrigued me as to this film it stars w...  \n",
       "49999  comments nearby are exceptionally well written...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6857b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1   \n",
      "1  Homelessness (or Houselessness as George Carli...      1   \n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1   \n",
      "3  This is easily the most underrated film inn th...      1   \n",
      "4  This is not the typical Mel Brooks film. It wa...      1   \n",
      "5  This isn't the comedic Robin Williams, nor is ...      1   \n",
      "6  Yes its an art... to successfully make a slow ...      1   \n",
      "7  In this \"critically acclaimed psychological th...      1   \n",
      "8  THE NIGHT LISTENER (2006) **1/2 Robin Williams...      1   \n",
      "9  You know, Robin Williams, God bless him, is co...      1   \n",
      "\n",
      "                                       filtered_text  \n",
      "0  is a cartoon ran at the same time as some othe...  \n",
      "1  as has been an issue for years but never a pla...  \n",
      "2  by dramatic lady have ever and love scenes in ...  \n",
      "3  is easily the most underrated film the its doe...  \n",
      "4  is not the typical was much less slapstick tha...  \n",
      "5  the comedic nor is it the of recent thriller i...  \n",
      "6  its an to successfully make a slow paced story...  \n",
      "7  this psychological thriller based on true a wr...  \n",
      "8  gives a is it about and is the near paranoia o...  \n",
      "9  is constantly shooting himself in the foot lat...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592d319",
   "metadata": {},
   "source": [
    "## Entraînement des vecteurs de mots (non-supervisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a28d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:22<00:00, 2207.17it/s]\n"
     ]
    }
   ],
   "source": [
    "beta = 50  # dimension des vecteurs de mots\n",
    "lambda_reg = 0.01\n",
    "nu_reg = 0.001\n",
    "epochs = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=list(vocab))\n",
    "X = vectorizer.transform(df[\"filtered_text\"])\n",
    "X = X.toarray()  # shape: (n_docs, vocab_size)\n",
    "\n",
    "n_docs, vocab_size = X.shape\n",
    "\n",
    "R = np.random.normal(0, 0.01, size=(beta, vocab_size))\n",
    "b = np.zeros(vocab_size)\n",
    "\n",
    "theta = np.random.normal(0, 0.01, size=(n_docs, beta))\n",
    "\n",
    "def softmax_probs(theta_k, R, b):\n",
    "    logits = np.dot(theta_k, R) + b\n",
    "    logits = logits - np.max(logits)  # stabilité numérique\n",
    "    exps = np.exp(logits)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_log_likelihood = 0.0\n",
    "    for k in tqdm(range(n_docs)):\n",
    "        x_k = X[k]  # comptage des mots\n",
    "        if x_k.sum() == 0:\n",
    "            continue\n",
    "        # E-step : optimiser θ_k\n",
    "        for _ in range(3):\n",
    "            probs = softmax_probs(theta[k], R, b)\n",
    "            grad_theta = R @ (x_k - probs * x_k.sum()) - lambda_reg * theta[k]\n",
    "            theta[k] += learning_rate * grad_theta\n",
    "\n",
    "        # Calcul du log-likelihood pour ce document\n",
    "        log_probs = np.dot(x_k, np.log(probs + 1e-9))  # pour éviter log(0)\n",
    "        total_log_likelihood += log_probs\n",
    "\n",
    "    for k in range(n_docs):\n",
    "        x_k = X[k]\n",
    "        if x_k.sum() == 0:\n",
    "            continue\n",
    "        probs = softmax_probs(theta[k], R, b)\n",
    "        err = x_k - probs * x_k.sum()\n",
    "        grad_R = np.outer(theta[k], err)\n",
    "        grad_b = err\n",
    "\n",
    "        # mise à jour immédiate\n",
    "        R += learning_rate * (grad_R - nu_reg * R)\n",
    "        b += learning_rate * grad_b\n",
    "\n",
    "\n",
    "    avg_ll = total_log_likelihood / n_docs\n",
    "    print(f\"[Epoch {epoch+1}] Avg semantic log-likelihood: {avg_ll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b054b4",
   "metadata": {},
   "source": [
    "# Ajustement par algorithme génétique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee25cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_probs(theta_k, R, b):\n",
    "    logits = np.dot(theta_k, R) + b\n",
    "    logits = logits - np.max(logits)  # stabilité numérique\n",
    "    exps = np.exp(logits)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, lambda_reg=0.1, nu_reg=0.1, learning_rate=0.01, beta=10):\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.nu_reg = nu_reg\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "\n",
    "    def train(self, vocab, filtered_texts, epochs=3):\n",
    "        vectorizer = CountVectorizer(vocabulary=list(vocab))\n",
    "        X = vectorizer.fit_transform(filtered_texts)\n",
    "        X = X.toarray()  # shape: (n_docs, vocab_size)\n",
    "\n",
    "        n_docs, vocab_size = X.shape\n",
    "\n",
    "        self.R = np.random.normal(0, 0.01, size=(self.beta, vocab_size))\n",
    "        self.b = np.zeros(vocab_size)\n",
    "\n",
    "        theta = np.random.normal(0, 0.01, size=(n_docs, self.beta))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            total_log_likelihood = 0.0\n",
    "            for k in tqdm(range(n_docs)):\n",
    "                x_k = X[k]\n",
    "                if x_k.sum() == 0:\n",
    "                    continue\n",
    "                # E-step : optimiser θ_k\n",
    "                for _ in range(3):\n",
    "                    probs = softmax_probs(theta[k], self.R, self.b)\n",
    "                    grad_theta = self.R @ (x_k - probs * x_k.sum()) - self.lambda_reg * theta[k]\n",
    "                    theta[k] += self.learning_rate * grad_theta\n",
    "\n",
    "                # Log-likelihood\n",
    "                log_probs = np.dot(x_k, np.log(probs + 1e-9))\n",
    "                total_log_likelihood += log_probs\n",
    "\n",
    "            for k in range(n_docs):\n",
    "                x_k = X[k]\n",
    "                if x_k.sum() == 0:\n",
    "                    continue\n",
    "                probs = softmax_probs(theta[k], self.R, self.b)\n",
    "                err = x_k - probs * x_k.sum()\n",
    "                grad_R = np.outer(theta[k], err)\n",
    "                grad_b = err\n",
    "\n",
    "                self.R += self.learning_rate * (grad_R - self.nu_reg * self.R)\n",
    "                self.b += self.learning_rate * grad_b\n",
    "\n",
    "            avg_ll = total_log_likelihood / n_docs\n",
    "            print(f\"[Epoch {epoch+1}] Avg semantic log-likelihood: {avg_ll:.4f}\")\n",
    "        \n",
    "        self.avg_log_likelihood = avg_ll\n",
    "\n",
    "    def compute_objective(self):\n",
    "        return self.avg_log_likelihood - self.lambda_reg * np.sum(self.R**2) - self.nu_reg * np.sum(self.b**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2605.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.0861\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2552.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -3178.9025\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:12<00:00, 2080.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -3136.9355\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2660.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.3828\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2655.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -943.2644\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2581.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -951.5969\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2670.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.0928\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:10<00:00, 2324.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -3190.9309\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:12<00:00, 2029.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -3144.3328\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2725.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.4154\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2700.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -957.4974\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2617.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -949.2766\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:06<00:00, 4014.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.3693\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:06<00:00, 4149.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -1065.2278\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:08<00:00, 2997.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -3170.7203\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:06<00:00, 4056.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.2569\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:06<00:00, 4001.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -3183.7785\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:08<00:00, 2957.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -3146.1465\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:06<00:00, 3991.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.2943\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:06<00:00, 3861.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -3183.5561\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:08<00:00, 2792.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -3196.8821\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:11<00:00, 2123.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1323.8899\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:12<00:00, 2016.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -3169.4263\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:14<00:00, 1696.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -3197.0487\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:35<00:00, 699.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1324.3545\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:35<00:00, 706.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg semantic log-likelihood: -943.5800\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:37<00:00, 666.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg semantic log-likelihood: -937.2420\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2557.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     65\u001b[39m         population = next_gen\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scored[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m best = \u001b[43mgenetic_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlen_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfiltered_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest individual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mgenetic_search\u001b[39m\u001b[34m(vocab, text, generations, population_size)\u001b[39m\n\u001b[32m     47\u001b[39m population = [generate_individual() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(population_size)]\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generations):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     scored = [(ind, \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[32m     51\u001b[39m     scored.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Maximize objective\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGeneration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Best score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscored[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(individual, vocab, text)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(individual, vocab, text):\n\u001b[32m     22\u001b[39m     model = Model(\n\u001b[32m     23\u001b[39m         beta=individual[\u001b[33m'\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     24\u001b[39m         lambda_reg=individual[\u001b[33m'\u001b[39m\u001b[33mlambda_reg\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     25\u001b[39m         nu_reg=individual[\u001b[33m'\u001b[39m\u001b[33mnu_reg\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     26\u001b[39m         learning_rate=individual[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     27\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.compute_objective()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, vocab, filtered_texts, epochs)\u001b[39m\n\u001b[32m     50\u001b[39m     grad_b = err\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.R += \u001b[38;5;28mself\u001b[39m.learning_rate * (grad_R - \u001b[38;5;28mself\u001b[39m.nu_reg * \u001b[38;5;28mself\u001b[39m.R)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28mself\u001b[39m.b += \u001b[38;5;28mself\u001b[39m.learning_rate * grad_b\n\u001b[32m     55\u001b[39m avg_ll = total_log_likelihood / n_docs\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Avg semantic log-likelihood: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_ll\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Espace de recherche des hyperparamètres\n",
    "param_space = {\n",
    "    'beta': [25, 50, 75, 100],\n",
    "    'lambda_reg': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    'nu_reg': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "# Génère un individu\n",
    "def generate_individual():\n",
    "    return {\n",
    "        'beta': random.choice(param_space['beta']),\n",
    "        'lambda_reg': random.choice(param_space['lambda_reg']),\n",
    "        'nu_reg': random.choice(param_space['nu_reg']),\n",
    "        'learning_rate': random.choice(param_space['learning_rate']),\n",
    "    }\n",
    "\n",
    "# Évalue un individu (doit appeler ton modèle ici)\n",
    "def evaluate(individual, vocab, text):\n",
    "    model = Model(\n",
    "        beta=individual['beta'],\n",
    "        lambda_reg=individual['lambda_reg'],\n",
    "        nu_reg=individual['nu_reg'],\n",
    "        learning_rate=individual['learning_rate']\n",
    "    )\n",
    "    model.train(vocab, text, epochs=3)\n",
    "    return model.compute_objective()  # À maximiser\n",
    "\n",
    "# Crossover\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    for key in parent1:\n",
    "        child[key] = random.choice([parent1[key], parent2[key]])\n",
    "    return child\n",
    "\n",
    "# Mutation\n",
    "def mutate(individual, mutation_rate=0.1):\n",
    "    for key in individual:\n",
    "        if random.random() < mutation_rate:\n",
    "            individual[key] = random.choice(param_space[key])\n",
    "    return individual\n",
    "\n",
    "# Algorithme principal\n",
    "def genetic_search(vocab, text, generations=10, population_size=10):\n",
    "    population = [generate_individual() for _ in range(population_size)]\n",
    "    \n",
    "    for generation in range(generations):\n",
    "        scored = [(ind, evaluate(ind, vocab, text)) for ind in population]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)  # Maximize objective\n",
    "\n",
    "        print(f\"\\nGeneration {generation + 1}, Best score: {scored[0][1]:.4f}\")\n",
    "        print(f\"Best individual: {scored[0][0]}\\n\")\n",
    "\n",
    "        survivors = [ind for ind, _ in scored[:population_size // 2]]\n",
    "\n",
    "        # Reproduction\n",
    "        next_gen = survivors.copy()\n",
    "        while len(next_gen) < population_size:\n",
    "            parents = random.sample(survivors, 2)\n",
    "            child = mutate(crossover(parents[0], parents[1]))\n",
    "            next_gen.append(child)\n",
    "\n",
    "        population = next_gen\n",
    "\n",
    "    return scored[0]\n",
    "\n",
    "best = genetic_search(vocab, df.iloc[:len_train][\"filtered_text\"], generations=10, population_size=10)\n",
    "print(f\"Best individual: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a357e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'beta': 75, 'lambda_reg': 0.0001, 'nu_reg': 0.0001, 'learning_rate': 0.01},\n",
       " np.float64(-917.4686275109037))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a0dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 14758/50000 [00:07<00:17, 2030.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m model = Model(\n\u001b[32m      2\u001b[39m     beta= \u001b[32m75\u001b[39m, \n\u001b[32m      3\u001b[39m     lambda_reg = \u001b[32m0.001\u001b[39m, \n\u001b[32m      4\u001b[39m     nu_reg = \u001b[32m0.0001\u001b[39m, \n\u001b[32m      5\u001b[39m     learning_rate = \u001b[32m0.001\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfiltered_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model.compute_objective()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, vocab, filtered_texts, epochs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# E-step : optimiser θ_k\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     probs = \u001b[43msoftmax_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     grad_theta = \u001b[38;5;28mself\u001b[39m.R @ (x_k - probs * x_k.sum()) - \u001b[38;5;28mself\u001b[39m.lambda_reg * theta[k]\n\u001b[32m     37\u001b[39m     theta[k] += \u001b[38;5;28mself\u001b[39m.learning_rate * grad_theta\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36msoftmax_probs\u001b[39m\u001b[34m(theta_k, R, b)\u001b[39m\n\u001b[32m      2\u001b[39m logits = np.dot(theta_k, R) + b\n\u001b[32m      3\u001b[39m logits = logits - np.max(logits)  \u001b[38;5;66;03m# stabilité numérique\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m exps = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m exps / np.sum(exps)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    beta= best[0]['beta'], \n",
    "    lambda_reg = best[0]['lambda_reg'], \n",
    "    nu_reg = best[0]['nu_reg'], \n",
    "    learning_rate = best[0]['learning_rate']\n",
    ")\n",
    "model.train(vocab, df.iloc[:len_train][\"filtered_text\"], epochs=3)\n",
    "model.compute_objective()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73adaac",
   "metadata": {},
   "source": [
    "# Test avec EarlyStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_probs(theta_k, R, b):\n",
    "    logits = np.dot(theta_k, R) + b\n",
    "    logits = logits - np.max(logits)  # stabilité numérique\n",
    "    exps = np.exp(logits)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "class ModelWithEarlyStopping:\n",
    "    def __init__(self, lambda_reg=0.1, nu_reg=0.1, learning_rate=0.01, beta=10):\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.nu_reg = nu_reg\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "\n",
    "    def train(self, vocab, filtered_texts, earlyStop=3, max_epochs=100):\n",
    "        vectorizer = CountVectorizer(vocabulary=list(vocab))\n",
    "        X = vectorizer.fit_transform(filtered_texts)\n",
    "        X = X.toarray()\n",
    "\n",
    "        n_docs, vocab_size = X.shape\n",
    "        self.R = np.random.normal(0, 0.01, size=(self.beta, vocab_size))\n",
    "        self.b = np.zeros(vocab_size)\n",
    "        theta = np.random.normal(0, 0.01, size=(n_docs, self.beta))\n",
    "\n",
    "        best_avg_ll = float('-inf')\n",
    "        best_params = {}\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{max_epochs}\")\n",
    "            total_log_likelihood = 0.0\n",
    "\n",
    "            for k in range(n_docs):\n",
    "                x_k = X[k]\n",
    "                if x_k.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # E-step : optimiser θ_k\n",
    "                for _ in range(3):\n",
    "                    probs = softmax_probs(theta[k], self.R, self.b)\n",
    "                    grad_theta = self.R @ (x_k - probs * x_k.sum()) - self.lambda_reg * theta[k]\n",
    "                    theta[k] += self.learning_rate * grad_theta\n",
    "\n",
    "                log_probs = np.dot(x_k, np.log(probs + 1e-9))\n",
    "                total_log_likelihood += log_probs\n",
    "\n",
    "            for k in range(n_docs):\n",
    "                x_k = X[k]\n",
    "                if x_k.sum() == 0:\n",
    "                    continue\n",
    "                probs = softmax_probs(theta[k], self.R, self.b)\n",
    "                err = x_k - probs * x_k.sum()\n",
    "                grad_R = np.outer(theta[k], err)\n",
    "                grad_b = err\n",
    "\n",
    "                self.R += self.learning_rate * (grad_R - self.nu_reg * self.R)\n",
    "                self.b += self.learning_rate * grad_b\n",
    "\n",
    "            avg_ll = total_log_likelihood / n_docs\n",
    "            print(f\"[Epoch {epoch+1}] Avg semantic log-likelihood: {avg_ll:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_ll > best_avg_ll + 1e-4:  # petite tolérance\n",
    "                best_avg_ll = avg_ll\n",
    "                best_params = {\n",
    "                    'R': self.R.copy(),\n",
    "                    'b': self.b.copy(),\n",
    "                    'theta': theta.copy()\n",
    "                }\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= earlyStop:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}. Best avg log-likelihood: {best_avg_ll:.4f}\")\n",
    "                    break\n",
    "\n",
    "        # Restore best params\n",
    "        self.R = best_params['R']\n",
    "        self.b = best_params['b']\n",
    "        self.theta = best_params['theta']\n",
    "        self.avg_log_likelihood = best_avg_ll\n",
    "\n",
    "    def compute_objective(self):\n",
    "        return self.avg_log_likelihood - self.lambda_reg * np.sum(self.R**2) - self.nu_reg * np.sum(self.b**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e955ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "[Epoch 1] Avg semantic log-likelihood: -1310.0769\n",
      "Epoch 2/100\n",
      "[Epoch 2] Avg semantic log-likelihood: -939.1448\n",
      "Epoch 3/100\n",
      "[Epoch 3] Avg semantic log-likelihood: -934.4805\n",
      "Epoch 4/100\n",
      "[Epoch 4] Avg semantic log-likelihood: -931.9069\n",
      "Epoch 5/100\n",
      "[Epoch 5] Avg semantic log-likelihood: -929.1385\n",
      "Epoch 6/100\n",
      "[Epoch 6] Avg semantic log-likelihood: -926.2104\n",
      "Epoch 7/100\n",
      "[Epoch 7] Avg semantic log-likelihood: -923.5449\n",
      "Epoch 8/100\n",
      "[Epoch 8] Avg semantic log-likelihood: -920.8669\n",
      "Epoch 9/100\n",
      "[Epoch 9] Avg semantic log-likelihood: -918.3053\n",
      "Epoch 10/100\n",
      "[Epoch 10] Avg semantic log-likelihood: -916.1097\n",
      "Epoch 11/100\n",
      "[Epoch 11] Avg semantic log-likelihood: -914.2885\n",
      "Epoch 12/100\n",
      "[Epoch 12] Avg semantic log-likelihood: -913.8383\n",
      "Epoch 13/100\n",
      "[Epoch 13] Avg semantic log-likelihood: -909.8718\n",
      "Epoch 14/100\n",
      "[Epoch 14] Avg semantic log-likelihood: -908.5739\n",
      "Epoch 15/100\n",
      "[Epoch 15] Avg semantic log-likelihood: -905.3351\n",
      "Epoch 16/100\n",
      "[Epoch 16] Avg semantic log-likelihood: -911.6369\n",
      "Epoch 17/100\n",
      "[Epoch 17] Avg semantic log-likelihood: -915.1444\n",
      "Epoch 18/100\n",
      "[Epoch 18] Avg semantic log-likelihood: -910.7679\n",
      "Early stopping at epoch 18. Best avg log-likelihood: -905.3351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-912.5670846563029)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWithEarlyStopping(\n",
    "    beta= best[0]['beta'], \n",
    "    lambda_reg = best[0]['lambda_reg'], \n",
    "    nu_reg = best[0]['nu_reg'], \n",
    "    learning_rate = best[0]['learning_rate']\n",
    ")\n",
    "model.train(vocab, df.iloc[:len_train][\"filtered_text\"])\n",
    "model.compute_objective()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd931d",
   "metadata": {},
   "source": [
    "# Modèle supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224701e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     exps = np.exp(logits)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exps / np.sum(exps)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mModelWithEarlyStopping\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnu_reg\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_sup\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mModelWithEarlyStopping\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m.alpha_sup = alpha_sup  \u001b[38;5;66;03m# pondération du terme supervisé\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab = vocab\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, filtered_texts, vocab=\u001b[38;5;28;43mself\u001b[39;49m.vocab, earlyStop=\u001b[32m3\u001b[39m, max_epochs=\u001b[32m100\u001b[39m):\n\u001b[32m     20\u001b[39m     vectorizer = CountVectorizer(vocabulary=\u001b[38;5;28mlist\u001b[39m(vocab))\n\u001b[32m     21\u001b[39m     X = vectorizer.fit_transform(filtered_texts).toarray()\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def softmax_probs(theta_k, R, b):\n",
    "    logits = np.dot(theta_k, R) + b\n",
    "    logits = logits - np.max(logits)  # stabilité numérique\n",
    "    exps = np.exp(logits)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "class ModelWithEarlyStopping:\n",
    "    def __init__(self, lambda_reg=0.1, nu_reg=0.1, learning_rate=0.01, beta=10, alpha_sup=1.0, vocab=None):\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.nu_reg = nu_reg\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.alpha_sup = alpha_sup  # pondération du terme supervisé\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def train(self, filtered_texts, vocab=None, earlyStop=3, max_epochs=100):\n",
    "        if vocab:\n",
    "            self.vocab = vocab  # pour usage futur\n",
    "        vocab = self.vocab\n",
    "        vectorizer = CountVectorizer(vocabulary=list(vocab))\n",
    "        X = vectorizer.fit_transform(filtered_texts).toarray()\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        n_docs, vocab_size = X.shape\n",
    "        self.R = np.random.normal(0, 0.01, size=(self.beta, vocab_size))\n",
    "        self.b = np.zeros(vocab_size)\n",
    "        theta = np.random.normal(0, 0.01, size=(n_docs, self.beta))\n",
    "\n",
    "        best_avg_ll = float('-inf')\n",
    "        best_params = {}\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{max_epochs}\")\n",
    "            total_log_likelihood = 0.0\n",
    "\n",
    "            for k in range(n_docs):\n",
    "                x_k = X[k]\n",
    "                if x_k.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                for _ in range(3):\n",
    "                    probs = softmax_probs(theta[k], self.R, self.b)\n",
    "                    grad_theta = self.R @ (x_k - probs * x_k.sum()) - self.lambda_reg * theta[k]\n",
    "                    theta[k] += self.learning_rate * grad_theta\n",
    "\n",
    "                log_probs = np.dot(x_k, np.log(probs + 1e-9))\n",
    "                total_log_likelihood += log_probs\n",
    "\n",
    "            for k in range(n_docs):\n",
    "                x_k = X[k]\n",
    "                if x_k.sum() == 0:\n",
    "                    continue\n",
    "                probs = softmax_probs(theta[k], self.R, self.b)\n",
    "                err = x_k - probs * x_k.sum()\n",
    "                grad_R = np.outer(theta[k], err)\n",
    "                grad_b = err\n",
    "\n",
    "                self.R += self.learning_rate * (grad_R - self.nu_reg * self.R)\n",
    "                self.b += self.learning_rate * grad_b\n",
    "\n",
    "            avg_ll = total_log_likelihood / n_docs\n",
    "            print(f\"[Epoch {epoch+1}] Avg semantic log-likelihood: {avg_ll:.4f}\")\n",
    "\n",
    "            if avg_ll > best_avg_ll + 1e-4:\n",
    "                best_avg_ll = avg_ll\n",
    "                best_params = {\n",
    "                    'R': self.R.copy(),\n",
    "                    'b': self.b.copy(),\n",
    "                    'theta': theta.copy()\n",
    "                }\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= earlyStop:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}. Best avg log-likelihood: {best_avg_ll:.4f}\")\n",
    "                    break\n",
    "\n",
    "        self.R = best_params['R']\n",
    "        self.b = best_params['b']\n",
    "        self.theta = best_params['theta']\n",
    "        self.avg_log_likelihood = best_avg_ll\n",
    "\n",
    "    def train_classifier(self, y):\n",
    "        \"\"\"\n",
    "        Entraîne une régression logistique binaire sur les vecteurs theta_k appris.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'theta'), \"Train the model first to get theta.\"\n",
    "        self.classifier = LogisticRegression()\n",
    "        self.classifier.fit(self.theta, y)\n",
    "        print(\"Régression logistique entraînée.\")\n",
    "\n",
    "    def fit(self, filtered_texts, y):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle et la régression logistique sur les textes filtrés et les labels.\n",
    "        \"\"\"\n",
    "        self.train(filtered_texts)\n",
    "        self.train_classifier(y)\n",
    "\n",
    "    def predict(self, filtered_texts):\n",
    "        \"\"\"\n",
    "        Prédit les labels binaires pour de nouveaux textes.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'classifier'), \"Train the classifier first.\"\n",
    "        X = self.vectorizer.transform(filtered_texts).toarray()\n",
    "        n_docs = X.shape[0]\n",
    "        theta = np.zeros((n_docs, self.beta))\n",
    "\n",
    "        for k in range(n_docs):\n",
    "            x_k = X[k]\n",
    "            if x_k.sum() == 0:\n",
    "                continue\n",
    "            theta_k = np.zeros(self.beta)\n",
    "            for _ in range(3):\n",
    "                probs = softmax_probs(theta_k, self.R, self.b)\n",
    "                grad_theta = self.R @ (x_k - probs * x_k.sum()) - self.lambda_reg * theta_k\n",
    "                theta_k += self.learning_rate * grad_theta\n",
    "            theta[k] = theta_k\n",
    "\n",
    "        preds = self.classifier.predict(theta)\n",
    "        return preds\n",
    "\n",
    "    def compute_objective(self):\n",
    "        return self.avg_log_likelihood - self.lambda_reg * np.sum(self.R**2) - self.nu_reg * np.sum(self.b**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59677d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d421ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithEarlyStopping(\n",
    "    beta= best[0]['beta'], \n",
    "    lambda_reg = best[0]['lambda_reg'], \n",
    "    nu_reg = best[0]['nu_reg'], \n",
    "    learning_rate = best[0]['learning_rate'],\n",
    "    vocab=vocab\n",
    ")\n",
    "model.fit(df.iloc[:len_train][\"filtered_text\"], df.iloc[:len_train][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Log-likelihood:\", model.compute_objective())\n",
    "preds = model.predict(df[\"filtered_text\"].iloc[len_train:])\n",
    "print(classification_report(df[\"label\"].iloc[len_train:], preds, target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf4e5e",
   "metadata": {},
   "source": [
    "## Ajustement avec supervision sentimentale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4785343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentiment Epoch 1] Loss: nan\n",
      "[Sentiment Epoch 2] Loss: nan\n",
      "[Sentiment Epoch 3] Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_docs):\n\u001b[0;32m     14\u001b[0m     x_k \u001b[38;5;241m=\u001b[39m X[k]\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x_k\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Représentation du doc = moyenne pondérée des vecteurs de mots\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joaki\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialisation des paramètres de la régression logistique\n",
    "psi = np.random.normal(0, 0.01, size=(beta,))\n",
    "bc = 0.0\n",
    "sentiment_lr = 0.1\n",
    "sentiment_epochs = 5\n",
    "\n",
    "# Normaliser les labels entre [0, 1]\n",
    "df[\"score\"] = df[\"label\"]\n",
    "\n",
    "# Entraînement simple\n",
    "for epoch in range(sentiment_epochs):\n",
    "    total_loss = 0\n",
    "    for k in range(n_docs):\n",
    "        x_k = X[k]\n",
    "        if x_k.sum() == 0:\n",
    "            continue\n",
    "        # Représentation du doc = moyenne pondérée des vecteurs de mots\n",
    "        doc_vec = (R @ x_k) / x_k.sum()\n",
    "        pred = expit(psi @ doc_vec + bc)\n",
    "        label = df[\"score\"].iloc[k]\n",
    "\n",
    "        # Gradient + update\n",
    "        error = label - pred\n",
    "        psi += sentiment_lr * error * doc_vec\n",
    "        bc += sentiment_lr * error\n",
    "\n",
    "        total_loss += - (label * np.log(pred + 1e-9) + (1 - label) * np.log(1 - pred + 1e-9))\n",
    "    print(f\"[Sentiment Epoch {epoch+1}] Loss: {total_loss/n_docs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003ea0b",
   "metadata": {},
   "source": [
    "## Comparaison des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80df492",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = X.copy()\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f42d8",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoW] Accuracy: 0.8471\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"[BoW] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066ef2c",
   "metadata": {},
   "source": [
    "#### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1a6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSA] Accuracy: 0.7704\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100)\n",
    "X_lsa_train = svd.fit_transform(X_train)\n",
    "X_lsa_test = svd.transform(X_test)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_lsa_train, y_train)\n",
    "y_pred = clf.predict(X_lsa_test)\n",
    "print(f\"[LSA] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587ffa6",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c073948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA] Accuracy: 0.8106\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=50, max_iter=10, random_state=42)\n",
    "X_lda_train = lda.fit_transform(X_train)\n",
    "X_lda_test = lda.transform(X_test)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_lda_train, y_train)\n",
    "y_pred = clf.predict(X_lda_test)\n",
    "print(f\"[LDA] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ae082",
   "metadata": {},
   "source": [
    "Modèle sémantique seul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic Only] Accuracy: 0.5035\n"
     ]
    }
   ],
   "source": [
    "def doc_features_from_R(X_data, R):\n",
    "    sums = X_data.sum(axis=1, keepdims=True)\n",
    "    sums[sums == 0] = 1  # évite division par zéro\n",
    "    return (X_data @ R.T) / sums\n",
    "\n",
    "X_r_train = doc_features_from_R(X_train, R)\n",
    "X_r_test = doc_features_from_R(X_test, R)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_r_train, y_train)\n",
    "y_pred = clf.predict(X_r_test)\n",
    "print(f\"[Semantic Only] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028abdc",
   "metadata": {},
   "source": [
    "#### Modèle complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic + Sentiment] Accuracy: 0.5035\n"
     ]
    }
   ],
   "source": [
    "def doc_sentiment_features(X_data, R, psi, bc):\n",
    "    feats = doc_features_from_R(X_data, R)\n",
    "    sentiment_score = expit(feats @ psi + bc).reshape(-1, 1)\n",
    "    return np.hstack([feats, sentiment_score])  # concat ψ info\n",
    "\n",
    "X_full_train = doc_sentiment_features(X_train, R, psi, bc)\n",
    "X_full_test = doc_sentiment_features(X_test, R, psi, bc)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_full_train, y_train)\n",
    "y_pred = clf.predict(X_full_test)\n",
    "print(f\"[Semantic + Sentiment] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d73f3",
   "metadata": {},
   "source": [
    "#### Concat BoW et modèle sémantique seul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic + BoW] Accuracy: 0.5035\n"
     ]
    }
   ],
   "source": [
    "X_comb_train = np.hstack([X_r_train, X_train])\n",
    "X_comb_test = np.hstack([X_r_test, X_test])\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_comb_train, y_train)\n",
    "y_pred = clf.predict(X_comb_test)\n",
    "print(f\"[Semantic + BoW] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a2195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
