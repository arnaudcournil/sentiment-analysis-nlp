{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09bc0586",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc67c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.11.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from tqdm->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joaki\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5548f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2a885",
   "metadata": {},
   "source": [
    "## Pré-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ee479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"aclImdb\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "def load_reviews_from_dir(directory, label):\n",
    "    data = []\n",
    "    for fname in os.listdir(directory):\n",
    "        if fname.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, fname), encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                data.append((text, label))\n",
    "    return data\n",
    "\n",
    "def load_all_data():\n",
    "    train_pos = load_reviews_from_dir(os.path.join(train_dir, \"pos\"), 1)\n",
    "    train_neg = load_reviews_from_dir(os.path.join(train_dir, \"neg\"), 0)\n",
    "    test_pos  = load_reviews_from_dir(os.path.join(test_dir,  \"pos\"), 1)\n",
    "    test_neg  = load_reviews_from_dir(os.path.join(test_dir,  \"neg\"), 0)\n",
    "    return train_pos + train_neg + test_pos + test_neg\n",
    "\n",
    "raw_data = load_all_data()\n",
    "df = pd.DataFrame(raw_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<br\\s*/?>\", \" \", text)  # remplace <br> et <br /> par un espace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # normalise les espaces\n",
    "    return text.strip()\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5050, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "vectorizer.fit(df[\"text\"])\n",
    "full_vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# --- Exclusion des 50 mots les plus fréquents comme dans l'article ---\n",
    "excluded_words = full_vocab[:50]\n",
    "vocab = full_vocab[50:]  # Top 5000 mots après exclusion des 50 premiers\n",
    "\n",
    "def filter_tokens(text, vocab_set):\n",
    "    tokens = text.split()\n",
    "    return \" \".join([tok for tok in tokens if tok in vocab_set])\n",
    "\n",
    "vocab_set = set(vocab)\n",
    "df[\"filtered_text\"] = df[\"text\"].apply(lambda x: filter_tokens(x, vocab_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6857b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1   \n",
      "1  Homelessness (or Houselessness as George Carli...      1   \n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1   \n",
      "3  This is easily the most underrated film inn th...      1   \n",
      "4  This is not the typical Mel Brooks film. It wa...      1   \n",
      "5  This isn't the comedic Robin Williams, nor is ...      1   \n",
      "6  Yes its an art... to successfully make a slow ...      1   \n",
      "7  In this \"critically acclaimed psychological th...      1   \n",
      "8  THE NIGHT LISTENER (2006) **1/2 Robin Williams...      1   \n",
      "9  You know, Robin Williams, God bless him, is co...      1   \n",
      "\n",
      "                                       filtered_text  \n",
      "0  is a cartoon ran at the same time as some othe...  \n",
      "1  as has been an issue for years but never a pla...  \n",
      "2  by dramatic lady have ever and love scenes in ...  \n",
      "3  is easily the most underrated film the its doe...  \n",
      "4  is not the typical was much less slapstick tha...  \n",
      "5  the comedic nor is it the of recent thriller i...  \n",
      "6  its an to successfully make a slow paced story...  \n",
      "7  this psychological thriller based on true a wr...  \n",
      "8  gives a is it about and is the near paranoia o...  \n",
      "9  is constantly shooting himself in the foot lat...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592d319",
   "metadata": {},
   "source": [
    "## Entraînement des vecteurs de mots (non-supervisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a28d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:28<00:00, 1745.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg semantic log-likelihood: -1310.0647\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:28<00:00, 1746.61it/s]\n"
     ]
    }
   ],
   "source": [
    "beta = 50  # dimension des vecteurs de mots\n",
    "lambda_reg = 0.01\n",
    "nu_reg = 0.001\n",
    "epochs = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=list(vocab))\n",
    "X = vectorizer.transform(df[\"filtered_text\"])\n",
    "X = X.toarray()  # shape: (n_docs, vocab_size)\n",
    "\n",
    "n_docs, vocab_size = X.shape\n",
    "\n",
    "R = np.random.normal(0, 0.01, size=(beta, vocab_size))\n",
    "b = np.zeros(vocab_size)\n",
    "\n",
    "theta = np.random.normal(0, 0.01, size=(n_docs, beta))\n",
    "\n",
    "def softmax_probs(theta_k, R, b):\n",
    "    logits = np.dot(theta_k, R) + b\n",
    "    logits = logits - np.max(logits)  # stabilité numérique\n",
    "    exps = np.exp(logits)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_log_likelihood = 0.0\n",
    "    for k in tqdm(range(n_docs)):\n",
    "        x_k = X[k]  # comptage des mots\n",
    "        if x_k.sum() == 0:\n",
    "            continue\n",
    "        # E-step : optimiser θ_k\n",
    "        for _ in range(3):\n",
    "            probs = softmax_probs(theta[k], R, b)\n",
    "            grad_theta = R @ (x_k - probs * x_k.sum()) - lambda_reg * theta[k]\n",
    "            theta[k] += learning_rate * grad_theta\n",
    "\n",
    "        # Calcul du log-likelihood pour ce document\n",
    "        log_probs = np.dot(x_k, np.log(probs + 1e-9))  # pour éviter log(0)\n",
    "        total_log_likelihood += log_probs\n",
    "\n",
    "    for k in range(n_docs):\n",
    "        x_k = X[k]\n",
    "        if x_k.sum() == 0:\n",
    "            continue\n",
    "        probs = softmax_probs(theta[k], R, b)\n",
    "        err = x_k - probs * x_k.sum()\n",
    "        grad_R = np.outer(theta[k], err)\n",
    "        grad_b = err\n",
    "\n",
    "        # mise à jour immédiate\n",
    "        R += learning_rate * (grad_R - nu_reg * R)\n",
    "        b += learning_rate * grad_b\n",
    "\n",
    "\n",
    "    avg_ll = total_log_likelihood / n_docs\n",
    "    print(f\"[Epoch {epoch+1}] Avg semantic log-likelihood: {avg_ll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf4e5e",
   "metadata": {},
   "source": [
    "## Ajustement avec supervision sentimentale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4785343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentiment Epoch 1] Loss: nan\n",
      "[Sentiment Epoch 2] Loss: nan\n",
      "[Sentiment Epoch 3] Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_docs):\n\u001b[0;32m     14\u001b[0m     x_k \u001b[38;5;241m=\u001b[39m X[k]\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x_k\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Représentation du doc = moyenne pondérée des vecteurs de mots\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joaki\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialisation des paramètres de la régression logistique\n",
    "psi = np.random.normal(0, 0.01, size=(beta,))\n",
    "bc = 0.0\n",
    "sentiment_lr = 0.1\n",
    "sentiment_epochs = 5\n",
    "\n",
    "# Normaliser les labels entre [0, 1]\n",
    "df[\"score\"] = df[\"label\"]\n",
    "\n",
    "# Entraînement simple\n",
    "for epoch in range(sentiment_epochs):\n",
    "    total_loss = 0\n",
    "    for k in range(n_docs):\n",
    "        x_k = X[k]\n",
    "        if x_k.sum() == 0:\n",
    "            continue\n",
    "        # Représentation du doc = moyenne pondérée des vecteurs de mots\n",
    "        doc_vec = (R @ x_k) / x_k.sum()\n",
    "        pred = expit(psi @ doc_vec + bc)\n",
    "        label = df[\"score\"].iloc[k]\n",
    "\n",
    "        # Gradient + update\n",
    "        error = label - pred\n",
    "        psi += sentiment_lr * error * doc_vec\n",
    "        bc += sentiment_lr * error\n",
    "\n",
    "        total_loss += - (label * np.log(pred + 1e-9) + (1 - label) * np.log(1 - pred + 1e-9))\n",
    "    print(f\"[Sentiment Epoch {epoch+1}] Loss: {total_loss/n_docs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003ea0b",
   "metadata": {},
   "source": [
    "## Comparaison des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d80df492",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = X.copy()\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f42d8",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f552fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoW] Accuracy: 0.8471\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"[BoW] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066ef2c",
   "metadata": {},
   "source": [
    "#### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88b1a6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSA] Accuracy: 0.7704\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100)\n",
    "X_lsa_train = svd.fit_transform(X_train)\n",
    "X_lsa_test = svd.transform(X_test)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_lsa_train, y_train)\n",
    "y_pred = clf.predict(X_lsa_test)\n",
    "print(f\"[LSA] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587ffa6",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c073948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA] Accuracy: 0.8106\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=50, max_iter=10, random_state=42)\n",
    "X_lda_train = lda.fit_transform(X_train)\n",
    "X_lda_test = lda.transform(X_test)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_lda_train, y_train)\n",
    "y_pred = clf.predict(X_lda_test)\n",
    "print(f\"[LDA] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ae082",
   "metadata": {},
   "source": [
    "Modèle sémantique seul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90d7fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic Only] Accuracy: 0.5035\n"
     ]
    }
   ],
   "source": [
    "def doc_features_from_R(X_data, R):\n",
    "    sums = X_data.sum(axis=1, keepdims=True)\n",
    "    sums[sums == 0] = 1  # évite division par zéro\n",
    "    return (X_data @ R.T) / sums\n",
    "\n",
    "X_r_train = doc_features_from_R(X_train, R)\n",
    "X_r_test = doc_features_from_R(X_test, R)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_r_train, y_train)\n",
    "y_pred = clf.predict(X_r_test)\n",
    "print(f\"[Semantic Only] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028abdc",
   "metadata": {},
   "source": [
    "#### Modèle complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eee6476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic + Sentiment] Accuracy: 0.5035\n"
     ]
    }
   ],
   "source": [
    "def doc_sentiment_features(X_data, R, psi, bc):\n",
    "    feats = doc_features_from_R(X_data, R)\n",
    "    sentiment_score = expit(feats @ psi + bc).reshape(-1, 1)\n",
    "    return np.hstack([feats, sentiment_score])  # concat ψ info\n",
    "\n",
    "X_full_train = doc_sentiment_features(X_train, R, psi, bc)\n",
    "X_full_test = doc_sentiment_features(X_test, R, psi, bc)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_full_train, y_train)\n",
    "y_pred = clf.predict(X_full_test)\n",
    "print(f\"[Semantic + Sentiment] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d73f3",
   "metadata": {},
   "source": [
    "#### Concat BoW et modèle sémantique seul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40af09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic + BoW] Accuracy: 0.5035\n"
     ]
    }
   ],
   "source": [
    "X_comb_train = np.hstack([X_r_train, X_train])\n",
    "X_comb_test = np.hstack([X_r_test, X_test])\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_comb_train, y_train)\n",
    "y_pred = clf.predict(X_comb_test)\n",
    "print(f\"[Semantic + BoW] Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a2195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
